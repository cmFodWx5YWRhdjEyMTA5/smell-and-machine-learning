{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import sys\n",
    "import re\n",
    "import scipy\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint as sp_randint\n",
    "from time import time\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_content(content_list):\n",
    "    source_code = []\n",
    "    content_list = eval(content_list)\n",
    "    for line in content_list:\n",
    "        # filter comments\n",
    "        if not re.match(\"\\s*\\/\\/\\s*isComment\", line):\n",
    "            source_code.append(line.replace(\"\\n\", \" newLine \"))\n",
    "    return ' '.join(source_code)\n",
    "\n",
    "def report(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")\n",
    "\n",
    "param_dist = {'C': scipy.stats.expon(scale=100),\n",
    "              'gamma': scipy.stats.expon(scale=.1),\n",
    "              'kernel': ['rbf', 'linear', 'poly'],\n",
    "              'class_weight':['balanced', None]}\n",
    "\n",
    "svm_classifier = svm.SVC(random_state=42)\n",
    "\n",
    "n_iter_search = 20\n",
    "random_search = RandomizedSearchCV(svm_classifier,\n",
    "                                   param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search,\n",
    "                                   cv=5,\n",
    "                                   n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"tmm\", \"lc\", \"dc\", \"lpl\", \"lm\"]\n",
    "for label in labels:\n",
    "    print(\"===== {} ==============\".format(label))\n",
    "    print(\"Reading data...\")\n",
    "    df = pd.read_csv('data/df/train_{}.csv'.format(label), engine=\"python\")\n",
    "    df_test = pd.read_csv('data/df/test_{}.csv'.format(label), engine=\"python\")\n",
    "\n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "    X_test = []\n",
    "    Y_test = []\n",
    "\n",
    "    print(\"Preparing lists...\")\n",
    "    for index, row in df.iterrows():\n",
    "        X_train.append(process_content(row[\"content\"]))\n",
    "        Y_train.append(row[\"smells\"])\n",
    "\n",
    "    for index, row in df_test.iterrows():\n",
    "        X_test.append(process_content(row[\"content\"]))\n",
    "        Y_test.append(row[\"smells\"])\n",
    "\n",
    "    print(\"Extracting features...\")\n",
    "    cv = CountVectorizer(binary=True)\n",
    "    cv.fit(X_train)\n",
    "    train_instances = cv.transform(X_train)\n",
    "    test_instances = cv.transform(X_test)\n",
    "    \n",
    "    X_t, X_v, y_t, y_v = train_test_split(train_instances, Y_train, train_size = 0.75)\n",
    "\n",
    "    start = time()\n",
    "    print(\"Hyperparameter tuning...\")\n",
    "    random_search.fit(train_instances, Y_train)\n",
    "    print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "      \" parameter settings.\" % ((time() - start), n_iter_search))\n",
    "    report(random_search.cv_results_)\n",
    "    print(\"============ EVALUATION on test set:\")\n",
    "    print(accuracy_score(Y_test, random_search.best_estimator_.predict(test_instances)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
